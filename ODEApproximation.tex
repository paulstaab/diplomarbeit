\section{The ODE Approximation}

Now as we have the well-posedness of the diffusion approximation, we will again consider a
deterministic model that approximates the stochastic process if the population size parameter $N$ is
large. As mentioned before, the SDE turns into an ODE if we increase $N$ towards infinity (we will
specify the type of this convergence later). Hence, we use the deterministic part of the SDE to
define the new model.

\begin{Definition}
We say a function $\mrOde: [0,\infty) \mapsto \Simplex$ with $\mrOdeT{t} = (\mrOdeKT{k}{t})_{k \in
\N}$ is an \emph{ODE Approximation of Muller's ratchet with
compensatory mutations}, if it is a solution of the ordinary differential equation on $\Simplex$ defined by
\begin{align} \label{ode:eq:def}\tag{$\ast\ast$}
  \dot \mrOdeComp_k & = 
  	-\alpha \Big( \sum_{\ell=0}^\infty (\ell - k) \mrOdeK{\ell} \Big) \mrOdeK{k} 
  	+ \lambda(\mrOdeK{k-1} - \mrOdeK{k}) 
  	+ \gamma ((k+1)\mrOdeK{k+1} - k \mrOdeK{k})
\end{align}
for all $k \in \N$ with $\mrOdeK{-1}:=0$.
\end{Definition}

\noindent
Similar to Chapter~3.2, we will show that the weak limit of the diffusion approximation for $N \to
\infty$ solves ODE \eqref{ode:eq:def} and that it is its only solution. Then, we will
explicitly calculate the solution and derive the long time behavior of the model. This will give us
the existence of a unique equilibrium point.


\subsection{Convergence And Uniqueness}

We start with the mentioned convergence that gives us existence of a solution as
well as proving the acclaimed approximation behavior.

\begin{Theorem} \label{ode:t:SDE->ODE}
Let $\lambda,\alpha \in [0,\infty),\gamma \in (0,\infty)$ and $\mrOdeT{0} \in \SimplexEM$. If
$\Vector{X}^{N_i} = \left(\Vector{X}^{N_i}(t)\right)_{t \geq 0}$ are solutions of the
SDE~\eqref{da:eq:difdef} starting in $\mrOdeT{0}$ for $N_1,N_2,\ldots \in \N$ with $N_i \to \infty$,
then their weak limit in the space of continuous $\SimplexEM$-valued paths $\mrOde = \left( \mrOdeT{t}\right)_{t \geq 0}$
exists and is a solution of the ODE~\eqref{ode:eq:def} almost surely.
\end{Theorem}

\begin{proof}
Note that $\Vector{X}^{N_i}$ is the solution of the 
$(\SimplexEM, \delta_{\underline x}, \GAsel + \GAmut + \GAcomp + G_{\text{res}}^{N_i},
\mathcal{F})$-martingale problem, while obviously any solution of the 
$(\SimplexEM, \delta_{\underline x}, \GAsel + \GAmut + \GAcomp, \mathcal{F})$-martingale
problem is a solution of differential equation~\eqref{ode:eq:def} almost surely. 
Since we have for all $f\in\mathcal F$ that
$$ \sup_{\underline x \in \mathbb S^\circ} 
|G_{\text{res}}^N f(\underline x)| \xrightarrow{N\to\infty} 0 $$
as \eqref{da:wq:Gresbounded} also holds for the Uniform norm on $\SimplexEM$ by
Lemma~\ref{da:l:expBounds}, the assertions follows if we can show the compact containment condition (see 
\cite{ethier_markov_2005}, Lemma~4.5.1 and Remark~4.5.2). That means,
we have to show that for all $\varepsilon>0$ and $T>0$ there is a
compact set $K_{\varepsilon, T}\subseteq \SimplexEM$ (with respect to the topology generated by
$d_P^\circ$) such that 
$$ \sup_{N} \mathbf P(\underline X^N(t) \in K^c_{\varepsilon, T} \text{ for a } t \in [0,T])\leq
\varepsilon.$$ Fix $\varepsilon>0$ and $T>0$. For $n\in\mathbb N$, there is $C_{n}$ by
Lemma~\ref{da:l:expBounds} such that
$$ \sup_{N} \mathbf P(\sup_{0\leq t\leq T} h_n(\underline X(t))>C_{n}) 
< \varepsilon 2^{-n}. $$ By Lemma~\ref{pre:l:poln}, a set $K \subseteq
\mathbb S^\circ$ is relatively compact if for all $\xi>0$ there is a
finite $C$ with $h_\xi(\underline x)\leq C$ for all $\underline x\in
K$. Hence, the closure of
$$ K_{\varepsilon, T} := \bigcap_{n=1}^\infty \{\underline x\in\mathbb S^\circ: h_n(\underline x) \leq C_{n}\}$$
is compact and
\begin{align*}
\sup_{N} \mathbf P(\underline X^N(t) \in K^c_{\varepsilon, T})\leq \sup_{N} 
\sum_{n=1}^\infty \mathbf P(\sup_{0\leq t\leq T} h_n(\underline X(t))>C_{n}) \leq
\varepsilon \sum_{n=1}^\infty 2^{-n} = \varepsilon.
\end{align*}  
\end{proof}

\noindent
To prove that ODE~\eqref{ode:eq:def} has no other solutions, we require some preliminary
computations which we carry out next. For $\underline x\in\mathbb S^\circ$, we refer to the function
$\xi\mapsto \log h_\xi(\underline x)$ as the cumulant generating function of $\underline x$.

\begin{Proposition}[Dynamics of cumulant generating function]\label{ode:p:dyn_cum}
For any solution $t\mapsto \mrOdeT{t}$ of \eqref{ode:eq:def} taking values in $\SimplexEM$, 
$$ \frac{d}{dt} \log h_\xi(\mrOdeT{t}) 
= \alpha \sum_{\ell =0}^\infty \ell x_\ell (t) + \lambda(e^\xi -1) - (\alpha +
  \gamma(1-e^{-\xi}))\frac{d}{d\xi} \log h_\xi(\mrOdeT{t}). $$
\end{Proposition}

\begin{proof}
Abbreviating $\mrOde := \mrOdeT{t}$, we compute
\begin{align*}
h_\xi(\underline x) \frac{d}{dt} & \log h_\xi(\mrOde) 
= \alpha \sum_{\ell =0}^\infty\sum_{k=0}^\infty (\ell -k)x_\ell  x_k e^{\xi k} +
  \lambda \sum_{k=0}^\infty (x_{k-1} -x_k)e^{\xi k} \\ 
& \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \gamma
  \sum_{k=0}^\infty ((k+1)x_{k+1} - kx_k)e^{\xi k} \\ 
& = \alpha \Big( \Big(\sum_{\ell =0}^\infty \ell  x_\ell \Big) h_\xi(\mrOde) -
  \frac{d}{d\xi} h_\xi(\underline x)\Big) + \lambda(e^\xi-1)
  h_\xi(\mrOde) - \gamma (1-e^{-\xi}) \frac{d}{d\xi}
  h_\xi(\mrOde)
\end{align*}
and so
\begin{align*}
\frac{d}{dt} \log h_\xi(\mrOde) & = \alpha \sum_{\ell=0}^\infty \ell x_\ell 
+ \lambda(e^\xi -1)- (\alpha + \gamma(1-e^{-\xi}))\frac{d}{d\xi} \log h_\xi(\mrOde).
\qedhere
\end{align*}
\end{proof}

\noindent
The equation in Proposition \ref{ode:p:dyn_cum} relates the time-derivative of 
$\log h_\xi(\underline x(t))$ with its $\xi$-derivative. Such a connection can
be very useful, for example when studying so-called duality for Markov processes (see
e.g.\ \cite{ethier_markov_2005}, p.\ 188ff). The next result is a special application of these ideas.

\begin{Corollary}[Duality]\label{ode:c:duality}
Let $t\mapsto \mrOdeT{t}$ and $t\mapsto \underline y(t)$ be two solutions of~\eqref{ode:eq:def}
taking values in $\SimplexEM$. Moreover $\xi: t \mapsto \xi(t)$ is the solution of 
$\xi' = -(\alpha + \gamma(1-e^{-\xi}))$, starting in some $\xi(0)$. Then,
\begin{align*}
\log h_{\xi(0)}(\underline x(t)) - \log h_{\xi(0)}(\underline y(t)) 
&= \log h_{\xi(t)}(\underline x(0)) - \log h_{\xi(t)}(\underline y(0)) \\ 
& \qquad \qquad \qquad + \int_0^t \sum_{\ell =0}^\infty \ell (x_\ell (s) - y_\ell (s)) ds.
  \end{align*}
\end{Corollary}

\begin{proof}
Using Proposition~\ref{ode:p:dyn_cum} and since for any differentiable function 
$g: \xi\mapsto g(\xi)$, 
$$\frac{d}{ds} g(\xi(t-s)) = (\alpha + \gamma(1-e^{-\xi(t-s)}))\frac{d}{d\xi} g(\xi(t-s)),  $$ 
we obtain
\begin{align*}
\frac{d}{ds} \Big(\log h_{\xi(t-s)}(\underline x(s)) & - \log
h_{\xi(t-s)}(\underline y(s))\Big) = \alpha \sum_{\ell =0}^\infty
\ell (x_\ell (s) - y_\ell (s)).
\end{align*}
Now the assertion follows by integrating.
\end{proof}

\noindent
We use this duality relation to show the uniqueness of
the solution of the differential equation.

\begin{Theorem} \label{ode:t:ode_uniqueness}
The solution of ODE~\eqref{ode:eq:def} is unique.
\end{Theorem}

\begin{proof}
If $\underline x(0) = \underline y(0)$, we obtain from
Corollary~\ref{ode:c:duality} that for all $\xi\in\mathbb R$ and any $t\geq
0$,
\begin{align}
  \label{eq:unique1}
  \log h_{\xi}(\underline x(t)) - \log h_{\xi}(\underline y(t)) =
  \int_0^t \sum_{\ell =0}^\infty \ell (x_\ell (s) - y_\ell (s)) ds.
\end{align}
Observing that only the left-hand side depends on $\xi$, this can only
be true if both sides are 0. Formally, taking derivatives with respect
to $\xi$ at $\xi=0$ the last equation gives
$$ \sum_{\ell =0}^\infty \ell (x_\ell (t) - y_\ell (t)) = 0.$$
Plugging this back into~\eqref{eq:unique1} gives
\begin{align}
  \label{eq:unique2}
  \log h_{\xi}(\underline x(t)) = \log h_{\xi}(\underline y(t)).
\end{align}
Since the function $\xi\mapsto \log h_\xi(\underline x)$ characterizes $\underline x \in \mathbb
S^\circ$ (e.g. see \cite{etheridge_how_2008}), we obtain that $\underline x(t) = \underline y(t)$.
\end{proof}

\subsection{Cumulants}
\label{ode:sec:cumulants}
Now we again turn our attention to equilibrium points. First we define
them for time-continuous processes as well. 

\begin{Definition}
We say $\Vector{x} \in \Simplex$ is an \emph{equilibrium point} of a
time-continuous Markov process $(\Vector{Z}(t))_{t \geq 0}$ on $\Simplex$ if
$$ \CE{\, \Vector{Z}(t)}{Z(0) = \Vector{x} \, } = \Vector{x} $$
for all $t \geq 0$. 
\end{Definition}

\noindent
In the proof of Theorem~\ref{da:t:SDE=MP}, we have seen that 
$$ \sum_{l \not= k} \int_0^t \sqrt{\frac{1}{N} \mrDifKT{k}{s} \mrDifKT{l}{s} }
  \, dW_{kl}(s) 
$$
is a martingale. Hence, the equilibrium points of the diffusion
approximation $\mrDif$ are again exactly the ones of the ODE approximation
$\mrOde$. Furthermore they are exactly the $\Vector{x} \in \Simplex$ with
$b_i(x) = 0$, where $b_i$ is as in \eqref{da:eq:difdef}.

\cite{depperschmidt_mathematical_2011} have identified one of these points by
using cumulants, which are the coefficients of the Taylor series of the cumulant generation function.

\begin{Definition}[Cumulants] \label{ode:d:cummulants}
Let $\Vector{x} \in \SimplexEM$. The cumulants $(\kappa_k(\Vector{x}))_{k = 1,2,\ldots}$ of $\Vector{x}$
are defined by the relation
\[
\log \sum_{k=0}^\infty x_k e^{\xi k}
= \sum_{k=1}^\infty \kappa_k(\Vector{x}) \frac{\xi^k}{k!} .
\]
\end{Definition}

\noindent
Note that the cumulants of $\Vector{x}$ are closely related to its moments. While $\kappa_1$ is
equal to the first moment of $\Vector{x}$, $\kappa_2$ is its variance (in agreement to our
notation in Proposition~\ref{da:p:com}). A general recursive formula for this relation is also known
in the literature (e.g. \cite{ledermann_probability_1981}). We have used before
that the cumulant generating function determines an $\Vector{x} \in \SimplexEM$. The cumulants also do so,
as they determine the cumulant generating function. For a more detailed review
of cumulants, see \cite{burger_moments_1991}.

Mainly because the cumulants of a Poisson distribution, say with parameter $\theta$, have 
the particularly nice form $\kappa_1 = \kappa_2 = \ldots = \theta$, cumulants
have proven to be useful for Muller's ratchet. The system~\eqref{ode:eq:def} translates nicely into cumulants as well.

\begin{Proposition}
Let $\mrOde = \mrOdeT{t}$ be the solution of the ODE~\eqref{ode:eq:def}. Then
\begin{align} \label{ode:eq:DeriveCum}
\frac{d}{dt} \kappa_k(\mrOde) = -\alpha \kappa_{k+1}(\mrOde) + \lambda + \gamma
\sum_{i=1}^{k} (-1)^{i+k-1} \binom{k}{i-1} \kappa_i(\mrOde)
\end{align}
\end{Proposition}

\begin{proof}
For this proof, we abbreviate $\kappa_k(\mrOdeT{t})$ with $\kappa_k$. Loosely following
\cite{depperschmidt_mathematical_2011} we notice that
\begin{align} \label{cm:IdKappaPreNotion}
\frac{ -\sum_{k=0}^\infty k \mrOdeK{k} e^{-\xi k} }
{ \sum_{k=0}^\infty \mrOdeK{k} e^{-\xi k} }
= \frac{d}{d \xi} \log \sum_{k=0}^\infty \mrOdeK{k} e^{-\xi k}
= \frac{d}{d \xi} \sum_{k=1}^\infty \kappa_k \frac{(-\xi)^k}{k!}
= - \sum_{k=0}^\infty \kappa_{k+1} \frac{(-\xi)^k}{k!}
\end{align}
and calculate
\begin{align} \nonumber
&  \frac{d}{d t} \sum_{k=1}^\infty \kappa_k \frac{(-\xi)^k}{k!}
= \frac{d}{d t} \left( \log \sum_{k=0}^\infty \mrOdeK{k} e^{-\xi k} \right)
=  \frac{ \sum_{k=0}^\infty e^{-\xi k} \frac{d}{d t} {\mrOdeK{k}}}
{\sum_{k=0}^\infty \mrOdeK{k} e^{-\xi k}} \\ \nonumber
& =  \frac{
\sum_{k=0}^\infty e^{-\xi k}
\left[
\left( \alpha \kappa_1 - \lambda \right) \mrOdeK{k}
- \left(\alpha + \gamma \right) k \mrOdeK{k}
+ \gamma (k+1) \mrOdeK{k+1}
+ \lambda \mrOdeK{k-1}
\right]
}
{\sum_{k=0}^\infty \mrOdeK{k} e^{-\xi k}} \\ \nonumber
& = \alpha \kappa_1 - \lambda +
\frac{
-(\alpha+\gamma) \sum_{k=0}^\infty k \mrOdeK{k} e^{-\xi k}
+ \gamma e^\xi \sum_{k=1}^\infty k \mrOdeK{k} e^{-\xi k}
}
{\sum_{k=0}^\infty \mrOdeK{k} e^{-\xi k}}
+ \lambda e^{-\xi} \\ \nonumber
& = \alpha \kappa_1 - \lambda -(\alpha+\gamma - \gamma e^\xi )
\frac{
\sum_{k=0}^\infty k \mrOdeK{k} e^{-\xi k}
}{
\sum_{k=0}^\infty \mrOdeK{k} e^{-\xi k}
}
+ \lambda e^{-\xi} \\ \nonumber
&\stackrel{\ref{cm:IdKappaPreNotion}}{=} \alpha \kappa_1 - \lambda -(\alpha+\gamma -
\gamma e^\xi )
\sum_{k=0}^\infty \kappa_{k+1}
\frac{ (-\xi)^k }{ k! }
+ \lambda e^{-\xi} \\ \label{cm:IdKappaCalc}
&=  -\alpha \sum_{k=1}^\infty \kappa_{k+1} \frac{ (-\xi)^k }{ k! }
+ \lambda (e^{- \xi} - 1)
+ \gamma (e^{\xi} - 1) \sum_{k=0}^\infty \kappa_{k+1} \frac{ (-\xi)^k }{ k! }.
\end{align}

\noindent
To identify the $\kappa_k$'s, we will now calculate the $n$-th derivative of the
right-hand side of \eqref{cm:IdKappaCalc}. To do so, we prove by induction that
\begin{align} \label{cm:IdKappaInduction}
\frac{d^n}{d^n \xi} \left( \gamma e^\xi \sum_{k=0}^\infty \kappa_{k+1}
\frac{(-\xi)^k}{k!} \right)
= \gamma e^\xi \sum_{k=0}^\infty \sum_{i=0}^n (-1)^i \binom{n}{i} \kappa_{k+i+1}
\frac{(-\xi)^k}{k!} .
\end{align}
First
\begin{align*}
\frac{d}{d \xi} \left( \gamma e^\xi \sum_{k=0}^\infty \kappa_{k+1}
\frac{(-\xi)^k}{k!} \right)
& = \gamma e^\xi \sum_{k=0}^\infty \kappa_{k+1}
\frac{(-\xi)^k}{k!} - \gamma e^\xi \sum_{k=0}^\infty \kappa_{k+2}
\frac{(-\xi)^k}{k!} \\
& = \gamma e^\xi \sum_{k=0}^\infty ( \kappa_{k+1} - \kappa_{k+2} )
\frac{(-\xi)^k}{k!} \\
& = \gamma e^\xi \sum_{k=0}^\infty \sum_{i=0}^1 (-1)^i \binom{1}{i} \kappa_{k+i+1}
\frac{(-\xi)^k}{k!}
\end{align*}
and
\begin{align*}
\frac{d^{n+1}}{d^{n+1} \xi} & \left( \gamma e^\xi \sum_{k=0}^\infty \kappa_{k+1}
\frac{(-\xi)^k}{k!} \right)
= \frac{d}{d \xi}
\left(
\gamma e^\xi \sum_{k=0}^\infty \sum_{i=0}^n (-1)^i
\binom{n}{i} \kappa_{k+i+1} \frac{(-\xi)^k}{k!}
\right) \\
& = \gamma e^\xi \sum_{k=0}^\infty \sum_{i=0}^n (-1)^i
\binom{n}{i} \kappa_{k+i+1} \frac{(-\xi)^k}{k!}
-
\gamma e^\xi \sum_{k=0}^\infty \sum_{i=0}^n (-1)^i
\binom{n}{i} \kappa_{k+i+2} \frac{(-\xi)^k}{k!} \\
& = \gamma e^\xi \sum_{k=0}^\infty \sum_{i=0}^{n+1} (-1)^i
\left( \binom{n}{i} + \binom{n}{i-1} \right ) \kappa_{k+i+1} \frac{(-\xi)^k}{k!}
\\
& = \gamma e^\xi \sum_{k=0}^\infty \sum_{i=0}^{n+1} (-1)^i \binom{n+1}{i}
\kappa_{k+i+1} \frac{(-\xi)^k}{k!} .
\end{align*}

\noindent
As the other summands are easy to derive, (\ref{cm:IdKappaInduction}) yields
\begin{align*}
&\frac{d^n}{d^n \xi}
\left(
-\alpha \sum_{k=1}^\infty \kappa_{k+1} \frac{ (-\xi)^k }{ k! }
+ \lambda (e^{- \xi} - 1)
+ \gamma (e^{\xi} - 1) \sum_{k=0}^\infty \kappa_{k+1} \frac{ (-\xi)^k }{ k! }
\right) \\ \nonumber
&= (-1)^n (-\alpha) \sum_{k=0}^\infty \kappa_{k+n+1} \frac{(-\xi)^k}{k!}
+ (-1)^n \lambda e^{- \xi}
+ \gamma e^\xi \sum_{k=0}^\infty \sum_{i=0}^n (-1)^i \binom{n}{i} \kappa_{k+i+1}
\frac{(-\xi)^k}{k!} \\ \nonumber
&\quad + (-1)^n (-\gamma) \sum_{k=0}^\infty \kappa_{k+n+1} \frac{(-\xi)^k}{k!}
\\ &= (-1)^n
\left[
-\alpha \sum_{k=0}^\infty \kappa_{k+n+1} \frac{(-\xi)^k}{k!}
+ \lambda e^{- \xi}
+ \gamma e^\xi \sum_{k=0}^\infty \sum_{i=1}^n (-1)^{i+n-1} \binom{n}{i-1}
\kappa_{k+i} \frac{(-\xi)^k}{k!}
\right]
\end{align*}

\noindent
Now deriving the left-hand side of (\ref{cm:IdKappaCalc}), we get
\begin{align*}
&\frac{d^n}{d^n \xi}
\left(
\frac{d}{d t} \sum_{k=1}^\infty \kappa_k \frac{(-\xi)^k}{k!}
\right)
= \frac{d^n}{d^n \xi}
\left(
\sum_{k=1}^\infty \frac{d}{d t} \kappa_k \frac{(-\xi)^k}{k!}
\right)
= (-1)^n \sum_{k=0}^\infty \frac{d}{d t} \kappa_{k+n} \frac{(-\xi)^k}{k!} .
\end{align*}
Evaluation of both functions at $\xi = 0$ finishes the proof.
\end{proof}

\noindent
Now, observe that for $\kappa_1 = \kappa_2 = \ldots = \theta$ the right-hand side of
\eqref{ode:eq:DeriveCum} equals
\begin{align*}
-\alpha \theta + \lambda - \gamma \theta = - (\alpha + \gamma) \theta + \lambda
\end{align*}
what yields the following corollary.

\begin{Corollary}\label{ode:c:ex_stat_point}
The Poisson distribution with parameter $\theta = \frac{\lambda}{\alpha +
\gamma}$ is an equilibrium point of $\mrOde$.
\end{Corollary}

\noindent
In fact, it is also its only equilibrium point. To prove this, we need to solve the
system~\eqref{ode:eq:def}.

\subsection{The Explicit Solution}
Using a stochastic particle system, we now explicitly calculate the solution of
ODE~\eqref{ode:eq:def}. Similar to the Maia-Botelho-Fontanari-Theorem, this is
the key result for this chapter.

\begin{Theorem} \label{ode:t:expl_solution}
The solution of the system~\eqref{ode:eq:def} is
\small
\begin{align} \label{ode:eq:expl_solution}
x_k(t) 
& = \frac{ \sum_{i=0}^\infty x_i(0) \sum_{j=0}^{i\wedge k}
    \binom{i}{j} \Big(\frac{\gamma}{\alpha+\gamma}\big(
    1-e^{-(\alpha+\gamma)t}\big)\Big)^{i-j}
    e^{-j(\alpha+\gamma)t} \frac{1}{(k-j)!}
    \Big(\frac{\lambda}{\alpha+\gamma} (1-e^{-(\alpha+\gamma)t})
    \Big)^{k-j}}{ \sum_{i=0}^\infty x_i(0)
    \Big(\frac{\gamma}{\alpha+\gamma} - \frac{\alpha}{\alpha+\gamma}
    e^{-(\alpha+\gamma)t}\Big)^i
    \exp\Big(\frac{\lambda}{\alpha+\gamma}(1-e^{-(\alpha+\gamma)t})
    \Big)}.
\end{align}
\end{Theorem}

\noindent
To prove this theorem, we define a Markov jump process that counts the
mutations of Muller's ratchet.

\begin{Definition}
\label{ode:d:jump_proc}
We define the process $(K_t)_{t\geq 0}$ as the pure Markov jump process that 
takes values in $\{\dagger, 0,1,2,...\}$, never leaves the state $\dagger$ and jumps
\begin{enumerate}
  \item from $k$ to $k+1$ at rate $\lambda$ and
  \item from $k$ to $k-1$ at rate $k\gamma$.
  \item In addition, the process is killed at $k$ with rate $\alpha k$,
    i.e.\ it jumps to $\dagger$.
\end{enumerate}
\end{Definition}

\noindent
This process is closely related to the system~\eqref{ode:eq:def} by the following proposition.

\begin{Proposition}[Particle representation]\label{ode:p:particle}
Let $\underline x(0) \in \mathbb S^\circ$ and $(K_t)_{t\geq 0}$ be
as in Definition~\ref{ode:d:jump_proc} with initial distribution given by
$\mathbf P[K_0=k] = x_k(0)$. 
Then,
\begin{align}\label{eq:ode:repr}
 x_k(t) := \mathbf P[K_t=k|K_t\neq \dagger]
\end{align}
solves the system in Theorem~\ref{ode:t:expl_solution}.
\end{Proposition}

\begin{proof}
From the definition of $(K_t)_{t\geq 0}$, it is clear that for small $\varepsilon>0$,
\small
\begin{align*}
x_k (t+\varepsilon) 
& = \frac{x_k(t)(1-\alpha k \varepsilon) + \lambda(x_{k-1}(t) - x_k(t)) \varepsilon + \gamma
    ((k+1)x_{k+1}(t) - kx_k(t)) \varepsilon}{1-\alpha \sum_j j
    x_j(t) \varepsilon} + \mathcal O(\varepsilon^2)\\ 
& = x_k(t) +
    \Big( -\alpha \Big(k x_k(t) - \sum_{j=0}^\infty j x_j(t)\Big)
    x_k(t) + \lambda(x_{k-1}(t) - x_k(t)) \\ 
& \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \gamma
    ((k+1)x_{k+1}(t) - k x_k(t))\Big)\varepsilon + \mathcal
    O(\varepsilon^2),
\end{align*}
\normalsize
which implies the result as $\varepsilon\to 0$.
\end{proof}

\noindent
Hence we can prove Theorem~\ref{ode:t:expl_solution} by computing $\CPR{K_t=k}{K_t\neq \dagger}$. To
do so, we need the following notation and a technical lemma.

\begin{Definition}
Denote by
\[ 
S_{l,k} := \left\{ \sigma \in S_l : 
	\sigma(1) < \ldots < \sigma(l-k), 
	\sigma(l-k+1) < \ldots < \sigma(l) 
	\right\}
\]
the set of all permutations of ${1,\ldots,l}$ with two groups of increasing elements of size
$l-k$ and $k$. 
\end{Definition}

\noindent
Note that, as the elements of one group determine the ones of the other, the size of $S_{l,k}$ is
\[ |S_{l,k}| = \binom{l}{k} = \binom{l}{k-l}. \]
The technical lemma basically states that the integrand has equal values on all polytopes gained by
connecting $l$ edges of an $l$-dimensional simplex that share a common vertice. 

\begin{Lemma} \label{ode:lem:sym}
Let $f,g: [0,\infty) \mapsto \R$ be bounded functions, $t \in [0,\infty)$ and
$l,k \in \N$ with $k \leq l$. We can calculate the following integral as
\begin{align*}
&\int\limits_{0 \leq t_1 \leq \ldots \leq t_l \leq t}
\sum_{\sigma \in S_{l,k}} 
	f(t_{\sigma(1)}) \cdots f(t_{\sigma(l-k)}) 
	g(t_{\sigma(l-k+1)}) \cdots g(t_{\sigma(l)})
	\, d(t_1,\ldots,t_l) \\
&= \quad \frac{1}{l!}\binom{l}{l-k} 	\left( \int_0^t f(t) dt \right)^{l-k} 
					\left( \int_0^t g(t) dt \right)^{k}
\end{align*}
\end{Lemma}
\begin{proof}
Once we have shown the above geometrical interpretation of the lemma, i.e. that
\small
\begin{equation} \label{ode:eq:permut}
\begin{aligned} 
&\int\limits_{0 \leq t_1 \leq \ldots \leq t_l \leq t}
\sum_{\sigma \in S_{l,k}} 
	f(t_{\sigma(1)}) \ldots f(t_{\sigma(l-k)}) 
	g(t_{\sigma(l-k+1)}) \ldots g(t_{\sigma(l)})
	\, d(t_1,\ldots,t_l) \\ 
= & \int\limits_{0 \leq t_{\tau(1)} \leq \ldots \leq t_{\tau(l)} \leq t}
	\sum_{\sigma \in S_{l,k}} 
	f(t_{\sigma(1)}) \ldots f(t_{\sigma(l-k)}) 
	g(t_{\sigma(l-k+1)}) \ldots g(t_{\sigma(l)})
	\, d(t_1,\ldots,t_l)
\end{aligned}
\end{equation}
\normalsize
holds for all $\tau \in S_l$, the assertion follows directly as 
\begin{align*}
&\int\limits_{0 \leq t_1 \leq \ldots \leq t_l \leq t}
\sum_{\sigma \in S_{l,k}} 
	f(t_{\sigma(1)}) \ldots f(t_{\sigma(l-k)}) 
	g(t_{\sigma(l-k+1)}) \ldots g(t_{\sigma(l)})
	d(t_1,\ldots,t_l) \\
= & \frac{1}{l!}\int\limits_{[0,t]^l}
\sum_{\sigma \in S_{l,k}} 
	f(t_{\sigma(1)}) \ldots f(t_{\sigma(l-k)}) 
	g(t_{\sigma(l-k+1)}) \ldots g(t_{\sigma(l)})
	d(t_1,\ldots,t_l) \\	
= &  \frac{1}{l!} \sum_{\sigma \in S_{l,k}} 
	\int\limits_{[0,t]^l}	
	f(t_{\sigma(1)}) \ldots f(t_{\sigma(l-k)}) 
	g(t_{\sigma(l-k+1)}) \ldots g(t_{\sigma(l)})
	d(t_1,\ldots,t_l) \\	
= &  \frac{1}{l!} \sum_{\sigma \in S_{l,k}} 
	\int\limits_{[0,t]^l}	
	f(t_{1}) \ldots f(t_{l-k}) 
	g(t_{l-k+1}) \ldots g(t_{l})
	d(t_1,\ldots,t_l) \\		
= & \frac{1}{l!}\binom{l}{l-k} 	\left( \int_0^t f(t) dt \right)^{l-k} 
				\left( \int_0^t g(t) dt \right)^{k},
\end{align*}
using Fubini's theorem in the last two steps as well as that $|S_l| = l!$ and $|S_{l,k}| =
\binom{l}{l-k}$.

\noindent
For \eqref{ode:eq:permut}, notice that for $\sigma \in S_{l,k}$ and $\tau \in S_l$, $\sigma \circ
\tau \not\in S_{l,k}$ in general. However, there exists exactly one permutation $\pi =
\pi_{\tau,\sigma} \in S_l$ that just sorts the elements of each $\sigma \circ \tau \left(\{
1,\ldots,l-k\}\right)$ and $\sigma \circ \tau \left(\{l-k+1,\ldots,l\}\right)$. Hence $\pi \circ
\sigma \circ \tau \in S_{l,k}$ and $\pi \circ \sigma \circ \tau \left((\{1,\ldots,l-k\}\right) = 
\sigma \circ \tau \left((\{1,\ldots,l-k\}\right)$. We additionally define the set
\[S^\tau_{l,k} := \{ \pi_{\tau,\sigma} \circ \sigma \circ \tau : \sigma \in S_{l,k}\}\]
that is a subset of $S_{l,k}$ as mentioned above. It is essential for the proof that $S_{l,k}
\subset S^\tau_{l,k}$ as well. Thus let $\tilde{\sigma} \in S^\tau_{l,k}$. Define $\sigma \in
S_l$ as \[ i \mapsto 
	\left\{ \begin{array}{cl} 
	m_i \left( \tilde{\sigma} \circ \tau^{-1}(\{1,\ldots,l-k\}) \right) & \mbox{if  }1 \leq i \leq l-k
	\\ 
	m_{i-l+k} \left( \tilde{\sigma} \circ \tau^{-1}(\{l-k+1,\ldots,l\}) \right) &
	\mbox{otherwise,} \end{array}\right.
\]
where $m_i$ denotes the $i$-th lowest number of a set. By definition, $\sigma$ obviously is an
element of $S_{l,k}$. Because $\sigma(A) = \tilde{\sigma} \circ \tau^{-1}(A)$, we have $\sigma\circ
\tau(A) = \tilde{\sigma}(A)$ for both $A = \{ 1,\ldots,l-k\}$ or $A = \{
l-k+1,\ldots,l\} $. Therefore, $\pi \circ \sigma \circ \tau = \tilde{\sigma}$.

\noindent
Hence, $S_{l,k} = S^\tau_{l,k}$ and therefore
\small
\begin{align*}
& \int\limits_{0 \leq t_{\tau(1)} \leq \ldots \leq t_{\tau(l)} \leq t}
	\sum_{\sigma \in S_{l,k}} 
	f(t_{\sigma(1)}) \ldots f(t_{\sigma(l-k)}) 
	g(t_{\sigma(l-k+1)}) \ldots g(t_{\sigma(l)})
	\,d(t_1,\ldots,t_l) \\
= & \int\limits_{0 \leq t_{1} \leq \ldots \leq t_{l} \leq t}
	\sum_{\sigma \in S_{l,k}} 
	f(t_{\sigma \circ \tau^{-1}(1)}) \ldots f(t_{\sigma \circ \tau^{-1}(l-k)}) 
	g(t_{\sigma \circ \tau^{-1}(l-k+1)}) \ldots g(t_{\sigma \circ \tau^{-1}(l)})
	\,d(t_1,\ldots,t_l) \\
= & \int\limits_{0 \leq t_{1} \leq \ldots \leq t_{l} \leq t}
	\sum_{\sigma \in S_{l,k}} 
	f(t_{\pi \circ \sigma \circ \tau^{-1}(1)}) \ldots f(t_{\pi \circ \sigma \circ \tau^{-1}(l-k)}) \\
  & \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad
	\cdot g(t_{\pi \circ \sigma \circ \tau^{-1}(l-k+1)}) \ldots g(t_{\pi \circ \sigma \circ
	\tau^{-1}(l)}) \,d(t_1,\ldots,t_l) \\
= & \int\limits_{0 \leq t_{1} \leq \ldots \leq t_{l} \leq t}
	\sum_{\sigma \in S^{\tau^{-1}}_{l,k}} 
	f(t_{\sigma(1)}) \ldots f(t_{\sigma(l-k)}) 
	g(t_{\sigma(l-k+1)}) \ldots g(t_{\sigma(l)})
	\,d(t_1,\ldots,t_l) \\
= & \int\limits_{0 \leq t_{1} \leq \ldots \leq t_{l} \leq t}
	\sum_{\sigma \in S_{l,k}} 
	f(t_{\sigma(1)}) \ldots f(t_{\sigma(l-k)}) 
	g(t_{\sigma(l-k+1)}) \ldots g(t_{\sigma(l)})
	\,d(t_1,\ldots,t_l) \qedhere
\end{align*}
\normalsize
\end{proof}

\noindent
We are now ready to prove Theorem~\ref{ode:t:expl_solution}.

\begin{proof}[Proof of Theorem~\ref{ode:t:expl_solution}]
We can directly compute the right hand side of \eqref{eq:ode:repr}. In order to do this, note that
the process $(K_t)_{t\geq 0}$ can be realized as follows: 
\begin{itemize}
\item Start with $K_0$ mutations, distributed according to
  $\mrOdeT{0}$.
\item New mutations arise at rate $\lambda$. 
\item Every mutation (present from the start or newly arisen) starts
  an exponential waiting time with parameter $\alpha+\gamma$. If this
  waiting time expires, then with probability
  $\frac{\alpha}{\alpha+\gamma}$ the process jumps to $\dagger$, and
  with the complementary probability $\frac{\gamma}{\alpha+\gamma}$
  the mutation disappears.
\end{itemize}
With $x_k(t)$ defined by \eqref{ode:eq:def}, we decompose the
probability of the event $\{K_t = k\}$ with respect to the number of
mutations present at time $0$. If $K_0=i$, a number $j\leq i \wedge k$
of these initial mutations are not compensated by time $t$ and the
remaining $i-j$ are compensated. In addition, a number $l\geq k-j$
mutations arise at times $0\leq t_1\leq \cdots \leq t_{l}\leq t$. From
these, $l-k+j$ are compensated and the remaining $k-j$ are not
compensated. These arguments lead to the following calculation, where
we write $\sim$ for equality up to factors not depending on $k$. The
first $\sim$ comes from the fact that the right hand side is the
unconditional probability $\mathbf P[K_t=k]$,

{\small
\begin{align*} 
&x_k(t) \sim \sum_{i=0}^\infty x_i(0) \sum_{j=0}^i \binom{i}{j}
  \Big(\frac{\gamma}{\alpha+\gamma}\big( 1-e^{-(\alpha+\gamma)t}\big)\Big)^{i-j} \cdot
  e^{-j(\alpha+\gamma)t} \\ 
& \qquad \qquad \sum_{l=k-j}^\infty \, \int\limits_{{0\leq t_1\leq\cdots \leq t_l} \leq t}
   \lambda^{l} e^{-\lambda t_1}e^{-\lambda(t_2-t_1)}
  \cdots e^{-\lambda(t_{l}-t_{l-1})} e^{-\lambda(t-t_{l})} \\
& \qquad \qquad \sum_{\sigma \in S_{l,(k-j)}}
  \prod_{r = 1}^{l-k+j} \frac{\gamma}{\alpha+\gamma}\big(
  1-e^{-(\alpha+\gamma)(t-t_{\sigma(r)})}\big) \cdot \prod_{s = l-k+j+1}^l
  e^{-(\alpha+\gamma)(t-t_{\sigma(s)})} \, d(t_1,...,t_{l})
  \\
& = \sum_{i=0}^\infty x_i(0) \sum_{j=0}^i \binom{i}{j}
  \Big(\frac{\gamma}{\alpha+\gamma}\big(
  1-e^{-(\alpha+\gamma)t}\big)\Big)^{i-j} \cdot e^{-j(\alpha+\gamma)t} \\ 
& \qquad \sum_{l=k-j}^\infty \frac{\lambda^l}{l!} e^{-\lambda t}
  \binom{l}{l-k+j}\Big( \frac{\gamma}{\alpha+\gamma}\int_0^t
  1-e^{-(\alpha+\gamma)(t-r)}dr\Big)^{l-k+j} 
  \Big(\int_0^te^{-(\alpha+\gamma)(t-s)}ds \Big)^{k-j} \\ 
& \sim \sum_{i=0}^\infty x_i(0) \sum_{j=0}^{i\wedge k} \binom{i}{j}
  \Big(\frac{\gamma}{\alpha+\gamma}\big(
  1-e^{-(\alpha+\gamma)t}\big)\Big)^{i-j} \cdot e^{-j(\alpha+\gamma)t}
  \frac{\lambda^{k-j}}{(k-j)!}
  \Big(\int_0^te^{-(\alpha+\gamma)(t-s)}ds \Big)^{k-j} \\ 
& \qquad \sum_{l=0}^\infty \frac{\lambda^l}{l!} \Big(
  \frac{\gamma}{\alpha+\gamma}\int_0^t
  1-e^{-(\alpha+\gamma)(t-r)}dr\Big)^{l} \\ 
& \sim \sum_{i=0}^\infty
  x_i(0) \sum_{j=0}^{i\wedge k} \binom{i}{j}
  \Big(\frac{\gamma}{\alpha+\gamma}\big(
  1-e^{-(\alpha+\gamma)t}\big)\Big)^{i-j} \cdot e^{-j(\alpha+\gamma)t}
  \frac{\lambda^{k-j}}{(k-j)!}  \Big(\frac{1}{\alpha+\gamma}
  (1-e^{-(\alpha+\gamma)t}) \Big)^{k-j}
\end{align*}}

\noindent
where the equality is Lemma~\ref{ode:lem:sym}. As we have ignored factors not depending on $k$ for
simplicity, we must ensure that $\Vector{x} \in \Simplex$. Hence, we have to divide through the sum
of the right-hand side, which is
{\small
\begin{align*}
  \sum_{i=0}^\infty x_i(0) & \sum_{j=0}^i \binom{i}{j}
  \Big(\frac{\gamma}{\alpha+\gamma}\big(
  1-e^{-(\alpha+\gamma)t}\big)\Big)^{i-j} \cdot e^{-j(\alpha+\gamma)t}
  \sum_{k=j}^\infty \frac{\lambda^{k-j}}{(k-j)!}  \Big(
  \frac{1}{\alpha+\gamma} (1-e^{-(\alpha+\gamma)t}) \Big)^{k-j} \\ & =
  \sum_{i=0}^\infty x_i(0) \Big(\frac{\gamma}{\alpha+\gamma} -
  \frac{\alpha}{\alpha+\gamma} e^{-(\alpha+\gamma)t}\Big)^i \cdot
  \exp\Big(\frac{\lambda}{\alpha+\gamma}(1-e^{-(\alpha+\gamma)t})
  \Big).
\end{align*}}
Hence, 
{\small
\begin{align*}
  x_k(t) & = \frac{ \sum_{i=0}^\infty x_i(0) \sum_{j=0}^{i\wedge k}
    \binom{i}{j} \Big(\frac{\gamma}{\alpha+\gamma}\big(
    1-e^{-(\alpha+\gamma)t}\big)\Big)^{i-j} 
    e^{-j(\alpha+\gamma)t} \frac{\lambda^{k-j}}{(k-j)!}
    \Big(\frac{1}{\alpha+\gamma} (1-e^{-(\alpha+\gamma)t})
    \Big)^{k-j}}{ \sum_{i=0}^\infty x_i(0)
    \Big(\frac{\gamma}{\alpha+\gamma} - \frac{\alpha}{\alpha+\gamma}
    e^{-(\alpha+\gamma)t}\Big)^i \cdot
    \exp\Big(\frac{\lambda}{\alpha+\gamma}(1-e^{-(\alpha+\gamma)t})
    \Big)}
\end{align*}}
\end{proof}

\noindent
Hence, we have solved the ODE approximation of Muller's ratchet with compensatory mutations. 
We can now easily derive that the ODE approximation also converges to a Poisson
distribution.

\begin{Corollary} \label{ode:c:conv_to_theta}
If either $\gamma > 0$ or $\mrOdeKT{0}{0} > 0$, then
\begin{align*} %\label{ode:c:conv}
    \mrOdeKT{k}{t} \xrightarrow{t \to \infty} e^{-\theta} \frac{\theta^k}{k!} 
    \quad \text{for } \quad \theta = \frac{\lambda}{\alpha + \gamma}.
\end{align*}
In particular, $\left( e^{-\theta} \frac{\theta^k}{k!} \right)_{k \in \N}$ is
the only equilibrium point of $\mrOde$ in this case.
\end{Corollary}
\begin{proof}
The proof is analogous to Corollary~\ref{ips:c:conv} if $\gamma = 0$ and
$\mrOdeKT{0}{0} > 0$. In the case $\gamma >0$, notice that all terms in the
numerator except that for $j = 0$ converge to $0$ as $t \to \infty$. Hence, we
have
\begin{align*}
\lim_{t \to \infty} \mrOdeKT{k}{t} 
&= \frac{\sum_{i=0}^\infty \left(\frac{\gamma}{\alpha + \gamma}\right)^i \cdot
\frac{\lambda^k}{k!}
\cdot
\frac{1}{(\alpha + \gamma)^k}} {\sum_{i=0}^\infty \left(\frac{\gamma}{\alpha +
\gamma}\right)^i \cdot \exp\left( \frac{\lambda}{\alpha + \gamma} \right)}
= e^{-\theta} \frac{\theta^k}{k!}. \qedhere
\end{align*}
\end{proof}

\noindent
Note that for $\gamma = 0$,
\begin{align*}
\mrOdeKT{0}{t} = \frac{\exp(- \frac{\lambda}{\alpha}(1-e^{-\alpha t}))}
					  {\sum_{j=0}^\infty \mrDifKT{j}{0} e^{-\alpha t j}}
				 \sum_{j=0}^\infty \frac{\frac{\lambda}{\alpha}(1-e^{-\alpha t})^{k-j}}
				 						{(k-j)!}
				 					\mrOdeKT{j}{0} e^{-\alpha t j}
\end{align*}
is equal to the large population limit of the discrete system \eqref{ips_explicit} if we replace
$e^{-\alpha t}$ with $(1-\alpha)^t$. There is a good chance that the
exploitation of this observation could lead to a solution for a time-discrete
deterministic system. However, such an examination would be beyond the scope of
the current manuscript.

Furthermore, our proof of Theorem~\ref{ode:t:expl_solution} relies on a positive selection
coefficient. However, it can be shown by deriving \eqref{ode:eq:expl_solution}
that the assertion is also true if $\alpha < 0$. In this situation, all
mutations are beneficial and $\gamma$ represents the probability that such a
mutation is lost again. The effects of a negative $\alpha$ on the stochastic
system also seems to be worth further investigation.
